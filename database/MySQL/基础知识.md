# MySQL基础知识



## 1、当前读

更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。  
除了update语句，select语句如果加锁，也是当前读。  
update t set t.count = t.count+1 where id=1  
select k from t where id=1 lock in share mode; 读锁（S锁，共享锁）  
select k from t for update; 写锁（X锁，排他锁）



## 2、MySQL基本架构

分为客户端，server层，存储引擎。  
其中server层包括连接器（管理连接，权限验证）、分析器（语法词法分析）、优化器（执行计划生成，索引选择）、执行器（表的操作权限验证，操作引擎，返回结果）和查询缓存。所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。MySQL8.0版本删除了查询缓存的功能。  
而存储引擎负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。InnoDB从5.5.5版本开始成为MySQL的默认存储引擎。



## 3、redo log重做日志和binlog归档日志

如果每次更新操作都要写进磁盘，就要先找到磁盘上对应的那条记录，然后再更新，整个过程IO成本，查找成本都很高。为了解决这个问题，MySQL使用了redo log。当有一条记录需要更新时，InnoDB引擎会先把记录写到redo log中，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作更新到磁盘里面。redo log是固定大小的，比如可以配置为一组4个文件，每个文件大小为1GB，从头开始写，写到末尾就又回到开头循环写。有了redo log，InnoDB就可以保证即使数据库异常重启，之前提交的记录都不会丢失，这个能力叫做crash-safe。redo log是InnoDB引擎特有的日志，而server层也有自己的日志，叫做binlog。

### redo log跟binlog有以下三个不同点

- 1、redo log是InnoDB引擎特有的，binlog是server层实现的。
- 2、redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的C字段加1”。
- 3、redo log是循环写的，空间固定会用完；binlog是可以追加写入的。

### 执行一条update语句时的流程 假如有张表有ID字段，更新ID=2这一行的C字段

- 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要从磁盘读入内存，然后再返回。
- 执行器拿到引擎给的行数据，把C的值加上1，再调用引擎接口写入这一行数据。
- 引擎将这行数据更新到内存中，同时将这个更新操作记录到redo log中，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。
- 执行器生成这个操作的binlog，并把binlog写入磁盘。
- 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成commit提交状态。



## 4、索引的常见模型

- 有序数组：数组查询速度很快，但是，在需要更新数据时成本太高。有序数组只适用于静态存储。
- 哈希表：将值放到数组中，用一个哈希函数将key换算成一个确定的位置，然后把value放在数组的固定位置上。哈希表这种结构适用于只有等值查询的场景。
- B+树：每一个索引在InnoDB里面对应一个B+树。
  在MySQL中，索引是在存储引擎层实现的，不同引擎的索引的工作方式并不一样。在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。索引分为主键索引和非主键索引。主键索引的叶子节点存的是整行数据。在InnoDB中，主键索引也叫聚簇索引（clustered index）。非主键索引的叶子节点内容是主键的值，非主键索引也被成为二级索引（secondary index）。主键索引的叶子节点存的是整行数据。非主键索引的叶子节点内容是主键的值。  

### 如果创建表时没有显示的定义主键，则InnoDB会按如下方式选择或创建主键：

- 首先判断表中有是否非空的唯一索引，如果有，则该列为主键。
- 如果不符合上述条件，InnoDB自动创建一个rowid作为主键。  

### 基于主键索引和普通索引的查询有什么区别？

- 如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID索引的B+树。
- 如果语句是select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID=500，再到ID索引树搜索一次，这个过程称为回表。  

### 如果你有一张证件表，证件是否可以当主键？ 

- 答：由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约20字节，而如果用自增主键，则只需要4个字节，如果是长整形，则是8个字节。显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间就越小。那有什么场景适合使用业务字段做主键呢？ 要求：只有一个索引，该索引必须是唯一索引。



## 5、锁

MySQL的锁大致可分成全局锁、表级锁和行锁三类： 

- 全局锁：对整个数据库实例加锁。命令 Flush tables with read lock。当你需要让整个库处于只读状态时，可以使用整个命令。全局锁的典型使用场景是做全库逻辑备份。也可以使用官方自带的逻辑备份工具mysqldump。当mysqldump使用参数-single-transaction时，导数据之前就会启动一个事务，生成一个一致性视图，而由于MVCC的支持，这个过程中数据是可以正常更新的。single-transaction方法只适用于所有的表使用事务引擎的库，即不能有MyISAM的引擎表。
- 表级锁：表级锁分两种，一个是表锁，一个是元数据锁。表锁的语法是lock tables ... read/write。可以用unlock tables主动释放锁。另一类表级的锁是元数据锁（MDL，metadata lock）。MDL锁不需要显示使用，在访问一个表的时候回自动加上。MDL的作用是，保证读写的正确性。当对一个表做增删改查操作时，加MDL读锁；当对一个表做结构变更时，加MDL写锁。读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。
- 行锁：行锁是在引擎层由各个引擎自己实现的。并不是所有的引擎都支持行锁，比如MyISAM就不支持行锁。不支持行锁则只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行。  在InnoDB中，行锁是在需要的时候才加上的，但并不是不需要了就立即释放，而是在事务结束时才释放。这个就是两阶段锁协议。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。



给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。如何安全的给表加字段避免数据库线程爆满？

- 由于MDL锁在事务提交时才会释放，为了避免MDL锁导致的等待，首先我们要解决长事务；
- 或者在alter table语句里面设置等待时间，如果在等待的时间内拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。



## 6、死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这些线程进入无限等待的状态，称为死锁。  

### 当出现死锁后，有两种策略

- 1、直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置，默认值是50秒。
- 2、发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务能够得以继续执行。将innodb_deadlock_delete设置为on，表示开启。  

### 怎么避免死锁检测的带来的性能问题？

- 1、关闭死锁检测（风险大 ）
- 2、控制并发度。如果在客户端做，客户端数量多时，效果不理想。在数据库服务端做，可以考虑在中间件实现。如果可以修改MySQL源码，也可以做在MySQL里面。基本思路是对于相同行的更新，在进入引擎前排队。
- 3、从业务设计上优化，比如影院账户，可以考虑放在多条记录上，影院的账户总额等于这10条记录的总和。这样每次给影院账户加金额时，随机选一条记录来加。



## 7、事务的隔离机制的实现 MVCC

在可重复读隔离级别下，启动事务有两种方式：

- begin/start transaction，在执行到第一个操作InnoDB表的语句，事务才真正启动。
- start transaction with consistent snapshot，会立刻启动一个事务。 

在可重复读隔离级别下，InnoDB里面每个事务都有一个唯一的事务id，叫做transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按照顺序严格递增的。

而每行的数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把这个transaction id赋值给这个数据版本的事务id，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。

也就是说，数据表中的一行记录，其实可能有多个版本，每个版本都有自己的row trx_id。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。

在实现上，InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在活跃的所有事务id。活跃指的是，启动了但还没提交。

数组里面事务id的最小值记为低水位，当前系统里面已经创建过的事务id的最大值加1记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。

这个视图数组把所有的 row trx_id 分成了几种不同的情况：

![images](https://github.com/FantasyLakers/my-lessons/blob/master/database/MySQL/%E6%95%B0%E6%8D%AE%E7%89%88%E6%9C%AC%E5%8F%AF%E8%A7%81%E6%80%A7%E8%A7%84%E5%88%99.png?raw=true)

这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：

- 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；

- 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；

- 如果落在黄色部分，那就包括两种情况

  - a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
  - b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

  

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

- 版本未提交，不可见
- 版本已提交，但是是在视图创建后提交的，不可见
- 版本已提交，而且是在视图创建前提交的，可见 



事务的可重复读的能力是怎么实现的？可重复读的核心就是一致性读（consistent read），用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

### 而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

- 在可重复读隔离级别下，只需要在事务开启时创建一致性视图，之后事务里的其他查询都共用这个一致性视图。查询只承认在事务启动前就已经提交完成的数据。
- 在读已提交隔离级别下，每一个语句都会重新算出一个新的视图。查询只承认在语句启动前就已经提交完成的数据。



## 8、主从复制

MySQL的主从复制默认是一个异步的复制过程，数据将从一个MySQL数据库（我们称之为Master）复制到另一个MySQL数据库（我们称之为Slave），在master与slave之间实现整个主从复制的过程是由三个线程参与完成的。其中两个线程（SQL线程和I/O线程）在slave端，另一个线程在master端（I/O线程）。  
要实现MySQL的主从复制，首先必须打开master端的binlog记录功能，整个实际的复制过程就是slave从master端获取binlog日志，然后再在slave上以相同顺序执行获取的binlog日志中记录的各种sql操作。  
从库IO在把信息放入到中继日志relay-log中后会触发sql线程，sql线程会把binlog中的sql语句解析出来变成sql语句放入数据库中。

三种主从复制模式

- 异步模式：这种模式下，主节点不会主动push binlog到从节点，这种会有延迟。
- 半同步模式：这种模式下主节点只需要接收到其中一台从节点的返回信息，就会commit；否则需要等待直到超时时间然后切换成异步模式再提交。
- 全同步：这种模式下需要所有从节点全部执行了commit并确认才会向客户端返回成功。

## 9、binlog格式

- statement格式：基于sql语句的模式，某些语句和函数如UUID，LOAD DATA INFILE等在复制过程中可能导致数据不一致甚至出错。由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：delete from table where a=xxx and b<yyy limit 1，在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 b，这就会导致数据不一致。
- row格式：基于行的模式，记录的是行的变化，很安全，但是binlog文件会比其他两种模式大很多，在一些大表中清除大量数据时会导致延迟变大。
- mixed格式：混合模式，根据语句来选用是statement模式还是row模式。  

为什么会有 mixed 这种 binlog 格式的存在场景？因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。  

当然现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row，这么做的最大好处就是：**恢复数据**。

## 10、MySQL是怎么保证数据不丢的？

只要 redo log 和 binlog 保证持久化到磁盘，就能确保 MySQL 异常重启后，数据可以恢复。

- binlog的写入机制：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。状态如图 1 所示。  ![images](https://github.com/FantasyLakers/my-lessons/blob/master/database/MySQL/binlog%E5%86%99%E5%85%A5%E6%AD%A5%E9%AA%A4.png?raw=true)   
  可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。  
  write 和 fsync 的时机，是由参数 sync_binlog 控制的：

  - sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；
  - sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；
  - sync_binlog=N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。  

  因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。  

- redo log的写入机制：事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。redo log buffer 里面的内容。 **是不是每次生成后都要直接持久化到磁盘呢？** 答案是，不需要。如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。 **那么，另外一个问题是，事务还没提交的时候，redo log buffer 中的部分日志有没有可能被持久化到磁盘呢？** 答案是，确实会有。这个问题，要从 redo log 可能存在的三种状态说起，这三种状态分别是：

  - 存在 redo log buffer 中，物理上是在 MySQL 进程内存中
  - 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面
  - 持久化到磁盘  

  日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：

  - 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;
  - 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；
  - 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。  

  InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。

  - 一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。
  - 另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。



## 11、普通索引和唯一索引

我们从查询和更新的角度上来分析下普通索引和唯一索引的性能上的差异

### 查询过程

假如现在由这样一条语句 select id from Table where k=5 ,其中k字段是索引，id字段是主键

- 普通索引：对于普通索引来说，查找到满足条件的第一个记录后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录
- 唯一索引：由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。你知道的，InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。

### 更新过程

先介绍一下**change buffer**：当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。

将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

**那么，什么条件下可以使用 change buffer 呢？**

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。因此，**唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用**。

如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。

第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB 的处理流程如下：

- 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束

第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下：

- 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

**将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。**

### change buffer 的使用场景

那么，现在有一个问题就是：普通索引的所有场景，使用 change buffer 都可以起到加速作用吗？因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。因此，对于**写多读少**的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。

### change buffer 和 redo log

现在，我们要在表上执行这个插入语句：insert into t(id,k) values(id1,k1),(id2,k2)

这里，我们假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图所示是带 change buffer 的更新状态图：

![images](https://github.com/FantasyLakers/my-lessons/blob/master/database/MySQL/%E5%B8%A6%20change%20buffer%20%E7%9A%84%E6%9B%B4%E6%96%B0%E8%BF%87%E7%A8%8B.png?raw=true)

这条更新语句做了如下的操作（按照图中的数字顺序）：

- Page 1 在内存中，直接更新内存
- Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息
- 将上述两个动作记入 redo log 中（图中 3 和 4）

做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。同时，图中的两个虚线箭头，是后台操作，不影响更新的响应时间。

那在这之后的读请求，要怎么处理呢？

- 读 Page 1 的时候，直接从内存返回。
- 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。

**可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存。所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。**



## 12、扫描行数是怎么判断的

MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。

那么，MySQL 是怎样得到索引的基数的呢？这里，我给你简单介绍一下 MySQL 采样统计的方法。为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：

- 设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10
- 设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16



## 13、怎么给字符串加索引

前缀索引：

- alter table SUser add index index1(email)：这条语句创建的 index1 索引里面，包含了每个记录的整个字符串
- alter table SUser add index index2(email(6))：而这条语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。

覆盖索引（id为主键）：

select id,email from SUser where email='zhangssxyz@xxx.com';

如果使用 index1（即 email 整个字符串的索引结构）的话，可以利用覆盖索引，从 index1 查到结果后直接就返回了，不需要回到 ID 索引再去查一次。而如果使用 index2（即 email(6) 索引结构）的话，就不得不回到 ID 索引再去判断 email 字段的值。即使你将 index2 的定义修改为 email(18) 的前缀索引，这时候虽然 index2 已经包含了所有的信息，但 InnoDB 还是要回到 id 索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。

总结怎么给字符串加索引：

- 直接创建完整索引，这样可能比较占用空间；
- 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
- 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题，比如身份证号字段；
- 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。



## 14、count(*)

首先要明确的是，在不同的 MySQL 引擎中，count(*) 有不同的实现方式

- MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高。如果加了 where 条件的话，MyISAM 表也是不能返回得这么快的。
- 而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。

为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？

这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。这里，我用一个算 count(*) 的例子来为你解释一下。

假设表 t 中现在有 10000 条记录，我们设计了三个用户并行的会话。

- 会话 A 先启动事务并查询一次表的总行数；
- 会话 B 启动事务，插入一行后记录后，查询表的总行数；
- 会话 C 先启动一个单独的语句，插入一行记录后，查询表的总行数

你会看到，在最后一个时刻，三个会话 A、B、C 会同时查询表 t 的总行数，但拿到的结果却不同。

这和 InnoDB 的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。

### 不同的 count 用法

在 select count(?) from t 这样的查询语句里面，count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能，有哪些差别？

**这里，首先弄清楚 count() 的语义。在InnoDB中，count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。所以，count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。**

- 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。
- 对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。
- 对于 count(字段) 来说：如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断字段不能为 null，按行累加；如果这个“字段”定义允许为 null，那么执行的时候，判断到字段有可能是 null，还要把值取出来再判断一下，不是 null 才累加。
- 但是 count(星) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。

所以结论是：按照效率排序的话，count(字段)<count(主键 id)<count(1)≈count()，所以我建议尽量使用count(*)。



## 15、order by

select city,name,age from t where city='杭州' order by name limit 1000

### 全字段排序

MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。通常情况下，这个语句执行流程如下所示 ：

- 1、初始化 sort_buffer，确定放入 name、city、age 这三个字段；
- 2、从索引 city 找到第一个满足 city='杭州’条件的主键 id；
- 3、到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；
- 4、从索引 city 取下一个记录的主键 id；
- 5、重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；
- 6、对 sort_buffer 中的数据按照字段 name 做快速排序；
- 7、按照排序结果取前 1000 行返回给客户端。

按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。

sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

### rowid 排序

在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。

所以如果单行很大，这个方法效率不够好。那么，如果 MySQL 认为排序的单行长度太大会怎么做呢？

SET max_length_for_sort_data = 16;

max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。

新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：

- 1、初始化 sort_buffer，确定放入两个字段，即 name 和 id；
- 2、从索引 city 找到第一个满足 city='杭州’条件的主键 id；
- 3、到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；
- 4、从索引 city 取下一个记录的主键 id；
- 5、重复步骤 3、4 直到不满足 city='杭州’条件为止，也就是图中的 ID_Y；
- 6、对 sort_buffer 中的数据按照字段 name 进行排序；
- 7、遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。



## 16、为什么索引失效

### 条件字段函数操作

select count(*) from tradelog where month(t_modified)=7

对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。由于在 t_modified 字段加了 month() 函数操作，导致了全索引扫描。

不过优化器在个问题上确实有“偷懒”行为，即使是对于不改变有序性的函数，也不会考虑使用索引。比如，对于 select * from tradelog where id + 1 = 10000 这个 SQL 语句，这个加 1 操作并不会改变有序性，但是 MySQL 优化器还是不能用 id 索引快速定位到 9999 这一行。所以，需要你在写 SQL 语句的时候，手动改写成 where id = 10000 -1 才可以。

### 隐式类型转换

select * from tradelog where tradeid=110717

交易编号 tradeid 这个字段上，本来就有索引，但是 explain 的结果却显示，这条语句需要走全表扫描。tradeid 的字段类型是 varchar(32)，而输入的参数却是整型，所以需要做类型转换。

那么，现在这里就有两个问题：数据类型转换的规则是什么？为什么有数据类型转换，就需要走全索引扫描？

**在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。**

对于优化器来说，这个语句相当于

select * from tradelog where CAST(tradid AS signed int) = 110717;

也就是说，这条语句触发了我们上面说到的规则：对索引字段做函数操作，优化器会放弃走树搜索功能。

### 隐式字符编码转换

假设系统里还有另外一个表 trade_detail，用于记录交易的操作细节。这时候，如果要查询 id=2 的交易的所有操作步骤信息，SQL 语句可以这么写：

select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2;

执行计划中显示：第一行显示优化器会先在交易记录表 tradelog 上查到 id=2 的行，这个步骤用上了主键索引，rows=1 表示只扫描一行；第二行 key=NULL，表示没有用上交易详情表 trade_detail 上的 tradeid 索引，进行了全表扫描。因为这两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引。

在这个执行计划里，是从 tradelog 表中取 tradeid 字段，再去 trade_detail 表里查询匹配字段。因此，我们把 tradelog 称为驱动表，把 trade_detail 称为被驱动表，把 tradeid 称为关联字段。**连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。**

优化后的语句：

select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 

主动把 l.tradeid 转成 utf8，就避免了被驱动表上的字符编码转换。



## 17、幻读



## 18、间隙锁和 next-key lock



